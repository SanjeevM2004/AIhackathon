{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94b97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce6458f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.43</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.53</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.82</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.88</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.49</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6946</th>\n",
       "      <td>6947</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.88</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6947</th>\n",
       "      <td>6948</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.34</td>\n",
       "      <td>812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6948</th>\n",
       "      <td>6949</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.43</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6949</th>\n",
       "      <td>6950</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.65</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6950</th>\n",
       "      <td>6951</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.83</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6951 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index  C1  C2  C3  C4  C5  C6  C7    C8    C9  Target\n",
       "0         1   3   9   9   0   6   0   1  0.60  0.43     342\n",
       "1         2   3   7  11   0   0   0   1  0.84  0.53     243\n",
       "2         3   1   1  23   0   4   1   2  0.40  0.82      91\n",
       "3         4   3   9   6   0   0   0   1  0.58  0.88      20\n",
       "4         5   2   5   9   0   6   0   1  0.54  0.49     218\n",
       "...     ...  ..  ..  ..  ..  ..  ..  ..   ...   ...     ...\n",
       "6946   6947   2   4   9   0   5   1   1  0.46  0.88     359\n",
       "6947   6948   2   5  17   0   5   1   1  0.66  0.34     812\n",
       "6948   6949   3   8  12   0   3   1   1  0.80  0.43     189\n",
       "6949   6950   1   2   7   0   2   1   1  0.24  0.65     100\n",
       "6950   6951   4  10   8   0   4   1   2  0.52  0.83     779\n",
       "\n",
       "[6951 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PS1A = pd.read_csv(\"PS1A_train.csv\")\n",
    "PS1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ebd2a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.43</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.53</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.82</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.88</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.49</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6946</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.88</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6947</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.34</td>\n",
       "      <td>812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6948</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.43</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6949</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.65</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6950</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.83</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6951 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      C1  C2  C3  C4  C5  C6  C7    C8    C9  Target\n",
       "0      3   9   9   0   6   0   1  0.60  0.43     342\n",
       "1      3   7  11   0   0   0   1  0.84  0.53     243\n",
       "2      1   1  23   0   4   1   2  0.40  0.82      91\n",
       "3      3   9   6   0   0   0   1  0.58  0.88      20\n",
       "4      2   5   9   0   6   0   1  0.54  0.49     218\n",
       "...   ..  ..  ..  ..  ..  ..  ..   ...   ...     ...\n",
       "6946   2   4   9   0   5   1   1  0.46  0.88     359\n",
       "6947   2   5  17   0   5   1   1  0.66  0.34     812\n",
       "6948   3   8  12   0   3   1   1  0.80  0.43     189\n",
       "6949   1   2   7   0   2   1   1  0.24  0.65     100\n",
       "6950   4  10   8   0   4   1   2  0.52  0.83     779\n",
       "\n",
       "[6951 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PS1A = PS1A.iloc[:, 1:]\n",
    "PS1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bb6a1b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanje\\AppData\\Local\\Temp\\ipykernel_12336\\4075742967.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_hat = mlp(torch.tensor(batch_X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 344.197, Validation Loss: 1368.165\n",
      "Epoch [2/150], Train Loss: 129.490, Validation Loss: 625.765\n",
      "Epoch [3/150], Train Loss: 94.940, Validation Loss: 493.836\n",
      "Epoch [4/150], Train Loss: 72.280, Validation Loss: 421.576\n",
      "Epoch [5/150], Train Loss: 59.933, Validation Loss: 371.599\n",
      "Epoch [6/150], Train Loss: 49.426, Validation Loss: 345.667\n",
      "Epoch [7/150], Train Loss: 46.475, Validation Loss: 329.510\n",
      "Epoch [8/150], Train Loss: 43.391, Validation Loss: 267.382\n",
      "Epoch [9/150], Train Loss: 42.523, Validation Loss: 255.053\n",
      "Epoch [10/150], Train Loss: 40.062, Validation Loss: 234.290\n",
      "Epoch [11/150], Train Loss: 38.057, Validation Loss: 229.854\n",
      "Epoch [12/150], Train Loss: 36.983, Validation Loss: 208.381\n",
      "Epoch [13/150], Train Loss: 35.554, Validation Loss: 200.423\n",
      "Epoch [14/150], Train Loss: 33.959, Validation Loss: 186.536\n",
      "Epoch [15/150], Train Loss: 32.917, Validation Loss: 192.349\n",
      "Epoch [16/150], Train Loss: 32.874, Validation Loss: 190.770\n",
      "Epoch [17/150], Train Loss: 32.285, Validation Loss: 194.489\n",
      "Epoch [18/150], Train Loss: 32.018, Validation Loss: 189.458\n",
      "Epoch [19/150], Train Loss: 31.226, Validation Loss: 190.693\n",
      "Epoch [20/150], Train Loss: 31.251, Validation Loss: 189.489\n",
      "Epoch [21/150], Train Loss: 30.654, Validation Loss: 188.507\n",
      "Epoch [22/150], Train Loss: 30.588, Validation Loss: 186.767\n",
      "Epoch [23/150], Train Loss: 29.879, Validation Loss: 183.807\n",
      "Epoch [24/150], Train Loss: 29.610, Validation Loss: 183.720\n",
      "Epoch [25/150], Train Loss: 29.201, Validation Loss: 180.934\n",
      "Epoch [26/150], Train Loss: 29.021, Validation Loss: 177.596\n",
      "Epoch [27/150], Train Loss: 28.665, Validation Loss: 174.783\n",
      "Epoch [28/150], Train Loss: 28.154, Validation Loss: 176.461\n",
      "Epoch [29/150], Train Loss: 28.247, Validation Loss: 172.686\n",
      "Epoch [30/150], Train Loss: 27.703, Validation Loss: 171.025\n",
      "Epoch [31/150], Train Loss: 27.589, Validation Loss: 168.914\n",
      "Epoch [32/150], Train Loss: 27.124, Validation Loss: 166.828\n",
      "Epoch [33/150], Train Loss: 26.997, Validation Loss: 169.310\n",
      "Epoch [34/150], Train Loss: 26.679, Validation Loss: 165.711\n",
      "Epoch [35/150], Train Loss: 26.367, Validation Loss: 165.914\n",
      "Epoch [36/150], Train Loss: 25.958, Validation Loss: 167.628\n",
      "Epoch [37/150], Train Loss: 25.762, Validation Loss: 168.170\n",
      "Epoch [38/150], Train Loss: 25.693, Validation Loss: 170.813\n",
      "Epoch [39/150], Train Loss: 25.702, Validation Loss: 170.144\n",
      "Epoch [40/150], Train Loss: 25.393, Validation Loss: 172.860\n",
      "Epoch [41/150], Train Loss: 25.462, Validation Loss: 169.471\n",
      "Epoch [42/150], Train Loss: 25.304, Validation Loss: 177.775\n",
      "Epoch [43/150], Train Loss: 25.729, Validation Loss: 179.348\n",
      "Epoch [44/150], Train Loss: 25.456, Validation Loss: 179.300\n",
      "Epoch [45/150], Train Loss: 25.621, Validation Loss: 191.849\n",
      "Epoch [46/150], Train Loss: 26.827, Validation Loss: 182.101\n",
      "Epoch [47/150], Train Loss: 29.014, Validation Loss: 167.163\n",
      "Epoch [48/150], Train Loss: 29.892, Validation Loss: 173.163\n",
      "Epoch [49/150], Train Loss: 29.149, Validation Loss: 158.126\n",
      "Epoch [50/150], Train Loss: 24.964, Validation Loss: 158.103\n",
      "Epoch [51/150], Train Loss: 23.450, Validation Loss: 150.067\n",
      "Epoch [52/150], Train Loss: 22.354, Validation Loss: 150.142\n",
      "Epoch [53/150], Train Loss: 22.045, Validation Loss: 153.934\n",
      "Epoch [54/150], Train Loss: 21.875, Validation Loss: 151.786\n",
      "Epoch [55/150], Train Loss: 23.107, Validation Loss: 150.110\n",
      "Epoch [56/150], Train Loss: 23.650, Validation Loss: 154.983\n",
      "Epoch [57/150], Train Loss: 23.681, Validation Loss: 151.153\n",
      "Epoch [58/150], Train Loss: 22.830, Validation Loss: 154.879\n",
      "Epoch [59/150], Train Loss: 22.293, Validation Loss: 145.810\n",
      "Epoch [60/150], Train Loss: 20.689, Validation Loss: 140.448\n",
      "Epoch [61/150], Train Loss: 20.798, Validation Loss: 148.390\n",
      "Epoch [62/150], Train Loss: 21.634, Validation Loss: 158.167\n",
      "Epoch [63/150], Train Loss: 21.164, Validation Loss: 152.615\n",
      "Epoch [64/150], Train Loss: 20.614, Validation Loss: 156.402\n",
      "Epoch [65/150], Train Loss: 20.428, Validation Loss: 148.994\n",
      "Epoch [66/150], Train Loss: 20.609, Validation Loss: 148.718\n",
      "Epoch [67/150], Train Loss: 20.594, Validation Loss: 147.542\n",
      "Epoch [68/150], Train Loss: 20.391, Validation Loss: 143.874\n",
      "Epoch [69/150], Train Loss: 19.746, Validation Loss: 141.585\n",
      "Epoch [70/150], Train Loss: 19.420, Validation Loss: 142.817\n",
      "Epoch [71/150], Train Loss: 19.470, Validation Loss: 144.905\n",
      "Epoch [72/150], Train Loss: 19.177, Validation Loss: 145.629\n",
      "Epoch [73/150], Train Loss: 18.860, Validation Loss: 157.928\n",
      "Epoch [74/150], Train Loss: 19.660, Validation Loss: 159.870\n",
      "Epoch [75/150], Train Loss: 19.673, Validation Loss: 146.575\n",
      "Epoch [76/150], Train Loss: 20.139, Validation Loss: 148.013\n",
      "Epoch [77/150], Train Loss: 20.224, Validation Loss: 154.468\n",
      "Epoch [78/150], Train Loss: 19.686, Validation Loss: 143.712\n",
      "Epoch [79/150], Train Loss: 18.956, Validation Loss: 135.106\n",
      "Epoch [80/150], Train Loss: 18.548, Validation Loss: 143.340\n",
      "Epoch [81/150], Train Loss: 19.249, Validation Loss: 157.176\n",
      "Epoch [82/150], Train Loss: 19.562, Validation Loss: 142.681\n",
      "Epoch [83/150], Train Loss: 18.229, Validation Loss: 148.442\n",
      "Epoch [84/150], Train Loss: 19.013, Validation Loss: 140.796\n",
      "Epoch [85/150], Train Loss: 18.579, Validation Loss: 141.233\n",
      "Epoch [86/150], Train Loss: 18.466, Validation Loss: 151.173\n",
      "Epoch [87/150], Train Loss: 18.857, Validation Loss: 152.152\n",
      "Epoch [88/150], Train Loss: 21.325, Validation Loss: 176.639\n",
      "Epoch [89/150], Train Loss: 23.111, Validation Loss: 171.774\n",
      "Epoch [90/150], Train Loss: 25.174, Validation Loss: 150.547\n",
      "Epoch [91/150], Train Loss: 24.545, Validation Loss: 142.985\n",
      "Epoch [92/150], Train Loss: 20.348, Validation Loss: 137.235\n",
      "Epoch [93/150], Train Loss: 17.713, Validation Loss: 139.426\n",
      "Epoch [94/150], Train Loss: 17.407, Validation Loss: 132.683\n",
      "Epoch [95/150], Train Loss: 17.183, Validation Loss: 134.260\n",
      "Epoch [96/150], Train Loss: 17.074, Validation Loss: 132.665\n",
      "Epoch [97/150], Train Loss: 16.946, Validation Loss: 135.731\n",
      "Epoch [98/150], Train Loss: 17.354, Validation Loss: 132.087\n",
      "Epoch [99/150], Train Loss: 17.435, Validation Loss: 129.350\n",
      "Epoch [100/150], Train Loss: 17.587, Validation Loss: 124.354\n",
      "Epoch [101/150], Train Loss: 17.516, Validation Loss: 122.822\n",
      "Epoch [102/150], Train Loss: 17.035, Validation Loss: 129.540\n",
      "Epoch [103/150], Train Loss: 17.260, Validation Loss: 129.270\n",
      "Epoch [104/150], Train Loss: 17.025, Validation Loss: 132.602\n",
      "Epoch [105/150], Train Loss: 16.461, Validation Loss: 131.573\n",
      "Epoch [106/150], Train Loss: 16.670, Validation Loss: 153.792\n",
      "Epoch [107/150], Train Loss: 17.592, Validation Loss: 148.109\n",
      "Epoch [108/150], Train Loss: 16.525, Validation Loss: 157.468\n",
      "Epoch [109/150], Train Loss: 18.160, Validation Loss: 156.279\n",
      "Epoch [110/150], Train Loss: 20.880, Validation Loss: 144.680\n",
      "Epoch [111/150], Train Loss: 20.524, Validation Loss: 143.098\n",
      "Epoch [112/150], Train Loss: 19.876, Validation Loss: 132.670\n",
      "Epoch [113/150], Train Loss: 18.001, Validation Loss: 139.663\n",
      "Epoch [114/150], Train Loss: 17.139, Validation Loss: 145.101\n",
      "Epoch [115/150], Train Loss: 16.291, Validation Loss: 151.663\n",
      "Epoch [116/150], Train Loss: 17.335, Validation Loss: 130.152\n",
      "Epoch [117/150], Train Loss: 17.869, Validation Loss: 127.582\n",
      "Epoch [118/150], Train Loss: 16.239, Validation Loss: 124.384\n",
      "Epoch [119/150], Train Loss: 16.581, Validation Loss: 149.223\n",
      "Epoch [120/150], Train Loss: 15.361, Validation Loss: 142.338\n",
      "Epoch [121/150], Train Loss: 14.810, Validation Loss: 121.435\n",
      "Epoch [122/150], Train Loss: 14.628, Validation Loss: 120.347\n",
      "Epoch [123/150], Train Loss: 13.720, Validation Loss: 116.949\n",
      "Epoch [124/150], Train Loss: 13.743, Validation Loss: 127.419\n",
      "Epoch [125/150], Train Loss: 13.626, Validation Loss: 132.577\n",
      "Epoch [126/150], Train Loss: 13.488, Validation Loss: 135.536\n",
      "Epoch [127/150], Train Loss: 14.556, Validation Loss: 126.453\n",
      "Epoch [128/150], Train Loss: 14.530, Validation Loss: 136.601\n",
      "Epoch [129/150], Train Loss: 15.696, Validation Loss: 132.384\n",
      "Epoch [130/150], Train Loss: 15.640, Validation Loss: 127.090\n",
      "Epoch [131/150], Train Loss: 15.516, Validation Loss: 125.405\n",
      "Epoch [132/150], Train Loss: 14.272, Validation Loss: 117.170\n",
      "Epoch [133/150], Train Loss: 14.235, Validation Loss: 128.734\n",
      "Epoch [134/150], Train Loss: 14.911, Validation Loss: 152.631\n",
      "Epoch [135/150], Train Loss: 15.758, Validation Loss: 150.533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [136/150], Train Loss: 14.627, Validation Loss: 166.422\n",
      "Epoch [137/150], Train Loss: 15.338, Validation Loss: 144.540\n",
      "Epoch [138/150], Train Loss: 16.541, Validation Loss: 130.380\n",
      "Epoch [139/150], Train Loss: 15.615, Validation Loss: 125.369\n",
      "Epoch [140/150], Train Loss: 13.806, Validation Loss: 117.781\n",
      "Epoch [141/150], Train Loss: 12.916, Validation Loss: 146.741\n",
      "Epoch [142/150], Train Loss: 13.091, Validation Loss: 139.991\n",
      "Epoch [143/150], Train Loss: 13.751, Validation Loss: 154.659\n",
      "Epoch [144/150], Train Loss: 13.954, Validation Loss: 122.262\n",
      "Epoch [145/150], Train Loss: 13.903, Validation Loss: 129.365\n",
      "Epoch [146/150], Train Loss: 13.176, Validation Loss: 133.594\n",
      "Epoch [147/150], Train Loss: 13.134, Validation Loss: 143.746\n",
      "Epoch [148/150], Train Loss: 12.453, Validation Loss: 128.899\n",
      "Epoch [149/150], Train Loss: 12.417, Validation Loss: 126.538\n",
      "Epoch [150/150], Train Loss: 13.134, Validation Loss: 123.303\n",
      "X1,  X2,  X3,  X4,  X5,  y\n",
      "0   -2   0   0   -1   0   0   0   0\n",
      "1   0   0   -1   -1   0   0   0   6\n",
      "0   0   -2   0   0   1   0   0   3\n",
      "0   0   0   1   0   0   0   0   -3\n",
      "0   0   1   -1   0   -1   0   0   2\n",
      "0   1   0   0   -1   0   0   0   0\n",
      "0   0   0   -2   0   1   0   0   6\n",
      "0   -1   1   -2   -1   0   0   -1   5\n",
      "2   1   0   -1   0   0   0   1   3\n",
      "1   -1   0   0   -1   0   0   0   0\n",
      "0   0   0   0   -1   2   0   0   7\n",
      "0   0   0   0   0   0   0   0   4\n",
      "0   0   0   0   0   0   0   0   3\n",
      "0   0   -1   0   0   0   1   0   1\n",
      "0   0   0   0   -1   0   0   0   3\n",
      "-1   0   0   0   0   0   0   -1   1\n",
      "1   -2   -2   0   0   0   1   1   4\n",
      "1   -1   0   1   1   -1   0   0   -1\n",
      "0   0   0   0   0   -1   0   0   1\n",
      "0   0   0   0   1   0   0   0   1\n",
      "0   0   0   -1   0   0   0   0   4\n",
      "0   0   0   0   0   0   0   0   4\n",
      "0   1   0   0   1   0   0   1   4\n",
      "0   0   1   0   0   1   0   0   3\n",
      "0   2   0   0   -2   0   0   -1   1\n",
      "0   2   0   0   0   1   1   0   2\n",
      "0   -2   0   2   1   0   0   0   0\n",
      "0   0   0   0   1   0   0   0   2\n",
      "0   0   0   0   1   1   2   1   3\n",
      "0   0   1   0   1   1   0   0   5\n",
      "0   0   -1   -1   0   0   0   1   3\n",
      "-2   0   0   0   0   0   -1   0   5\n",
      "1   -2   0   0   1   0   -1   0   0\n",
      "0   0   0   2   0   1   0   0   0\n",
      "0   0   0   0   0   0   0   0   3\n",
      "0   -1   1   -1   0   -1   0   1   2\n",
      "0   0   1   0   -1   0   0   0   2\n",
      "0   1   0   -1   0   0   -1   0   1\n",
      "-1   0   0   1   -1   -1   -1   0   -2\n",
      "0   0   0   0   0   -1   0   -1   0\n",
      "0   1   0   0   0   0   0   0   4\n",
      "1   0   0   -1   0   0   -1   0   3\n",
      "-1   0   0   0   0   0   0   -1   0\n",
      "1   0   0   0   0   0   0   -1   2\n",
      "0   -1   -2   -1   0   0   0   0   6\n",
      "0   1   0   0   0   1   1   0   3\n",
      "0   -1   0   1   0   2   1   0   3\n",
      "0   0   0   1   0   -1   0   0   -3\n",
      "-2   0   -2   0   0   0   0   0   3\n",
      "1   0   0   2   -1   0   0   1   0\n",
      "0   -1   -1   2   0   0   0   0   -3\n",
      "0   0   0   0   -1   0   0   0   2\n",
      "0   0   0   0   0   0   -1   0   0\n",
      "1   0   0   -2   0   0   1   0   5\n",
      "1   -1   -2   1   0   0   -1   0   1\n",
      "1   0   0   1   0   1   0   0   2\n",
      "-1   0   1   0   0   0   -1   -1   3\n",
      "0   -1   0   0   0   -1   0   0   -2\n",
      "0   0   0   2   0   0   -1   0   -2\n",
      "-1   -1   0   0   -2   0   -2   0   5\n",
      "0   0   -2   1   -1   0   0   0   -2\n",
      "-1   0   -1   1   0   -1   0   0   -3\n",
      "0   1   0   0   0   0   -1   0   4\n",
      "0   1   1   -1   0   0   0   0   3\n",
      "1   0   0   0   0   -1   0   0   1\n",
      "2   1   -1   0   1   0   1   0   3\n",
      "0   0   -1   0   -2   0   1   -1   4\n",
      "0   -1   0   0   0   0   -1   0   4\n",
      "0   0   2   1   -1   0   0   0   2\n",
      "0   0   0   1   0   0   0   -2   0\n",
      "0   1   0   0   0   1   1   0   4\n",
      "1   0   -2   0   0   0   2   0   2\n",
      "0   0   -1   0   0   1   0   1   4\n",
      "1   1   0   0   1   0   0   0   3\n",
      "0   0   0   -2   -1   0   0   0   5\n",
      "0   0   0   -2   0   0   -1   -1   6\n",
      "0   2   -1   -1   0   0   0   0   4\n",
      "0   0   0   0   0   1   0   0   3\n",
      "1   -3   0   0   0   0   0   0   1\n",
      "-2   -1   0   0   -1   0   0   1   2\n",
      "-1   1   0   0   0   1   1   1   4\n",
      "0   2   0   0   0   -1   0   0   0\n",
      "0   0   0   0   -1   0   0   0   1\n",
      "0   0   -1   0   0   0   0   -1   0\n",
      "-1   0   0   0   0   0   0   0   2\n",
      "-2   1   0   1   0   0   -1   -1   0\n",
      "0   0   0   0   0   0   0   0   2\n",
      "-1   0   3   0   0   0   0   1   3\n",
      "-1   1   0   0   0   0   1   1   0\n",
      "-1   0   2   0   0   0   -1   0   2\n",
      "0   0   0   0   1   0   0   1   2\n",
      "0   2   0   0   0   0   0   0   1\n",
      "0   -1   1   1   1   1   0   0   3\n",
      "0   0   0   0   0   0   0   0   2\n",
      "0   0   -1   0   0   0   0   -1   4\n",
      "0   0   0   0   1   0   0   0   3\n",
      "0   0   0   0   0   0   1   0   1\n",
      "0   0   0   -1   0   0   -1   0   5\n",
      "0   0   0   1   0   -2   -1   0   -2\n",
      "0   0   -2   1   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the hyperparameters\n",
    "n_samples = 1000\n",
    "n_features = 5\n",
    "hidden_size_1 = 256\n",
    "learning_rate = 0.01\n",
    "epochs = 150\n",
    "batch_size = 32\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, hidden_size_1)\n",
    "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_1)\n",
    "        self.fc3 = nn.Linear(hidden_size_1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = (self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(y_hat, y):\n",
    "    MSE = F.mse_loss(y_hat, y, reduction='sum')\n",
    "    return MSE\n",
    "\n",
    "# Generate synthetic data\n",
    "X = torch.tensor(np.array(PS1A.iloc[:, :9]))\n",
    "X = X.to(torch.float32)\n",
    "w_true = torch.randn(9, 1, dtype= torch.float32)\n",
    "b_true = torch.randn(1, 1, dtype=torch.float32)\n",
    "y = torch.mm(X, w_true) + b_true + torch.randn(len(X), 1, dtype=torch.float32)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_size = int(0.8 * n_samples)\n",
    "val_size = n_samples - train_size\n",
    "train_indices = torch.randperm(n_samples)[:train_size]\n",
    "val_indices = torch.randperm(n_samples)[train_size:]\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_val = X[val_indices]\n",
    "y_val = y[val_indices]\n",
    "\n",
    "# Initialize the MLP and optimizer\n",
    "mlp = MLP()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the MLP\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for batch_start in range(0, train_size, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, train_size)\n",
    "        batch_X = X_train[batch_start:batch_end].to(torch.float32)\n",
    "        batch_y = y_train[batch_start:batch_end]\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = mlp(torch.tensor(batch_X))\n",
    "        loss = loss_function(y_hat, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * (batch_end - batch_start)\n",
    "    train_loss /= train_size\n",
    "    train_losses.append(train_loss)\n",
    "    with torch.no_grad():\n",
    "        y_val_hat = mlp(X_val)\n",
    "        val_loss = loss_function(y_val_hat, y_val)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    print('Epoch [{}/{}], Train Loss: {:.3f}, Validation Loss: {:.3f}'.format(epoch+1, epochs, train_loss, val_loss.item()))\n",
    "\n",
    "# Generate synthetic data using the trained model\n",
    "X_test = torch.randn(len(X), 9, dtype=torch.float32)\n",
    "y_test = mlp(X_test).detach()\n",
    "\n",
    "# Print the first few rows of the synthetic data\n",
    "print('X1,  X2,  X3,  X4,  X5,  y')\n",
    "for i in range(100):\n",
    "    print(int(X_test[i, 1]), ' ',int(X_test[i, 2]),' ', int(X_test[i, 3]),' ', int(X_test[i, 4]),' ', int(X_test[i, 5]),' ', int(X_test[i, 6]), ' ',int(X_test[i, 7]),' ', int(X_test[i, 8]),' ',int(y_test[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5dfd38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "PS1B = pd.read_csv(\"PS1A_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf10c9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.43</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.53</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.82</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.88</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.49</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6946</th>\n",
       "      <td>6947</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.88</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6947</th>\n",
       "      <td>6948</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.34</td>\n",
       "      <td>812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6948</th>\n",
       "      <td>6949</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.43</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6949</th>\n",
       "      <td>6950</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.65</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6950</th>\n",
       "      <td>6951</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.83</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6951 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index  C1  C2  C3  C4  C5  C6  C7    C8    C9  Target\n",
       "0         1   3   9   9   0   6   0   1  0.60  0.43     342\n",
       "1         2   3   7  11   0   0   0   1  0.84  0.53     243\n",
       "2         3   1   1  23   0   4   1   2  0.40  0.82      91\n",
       "3         4   3   9   6   0   0   0   1  0.58  0.88      20\n",
       "4         5   2   5   9   0   6   0   1  0.54  0.49     218\n",
       "...     ...  ..  ..  ..  ..  ..  ..  ..   ...   ...     ...\n",
       "6946   6947   2   4   9   0   5   1   1  0.46  0.88     359\n",
       "6947   6948   2   5  17   0   5   1   1  0.66  0.34     812\n",
       "6948   6949   3   8  12   0   3   1   1  0.80  0.43     189\n",
       "6949   6950   1   2   7   0   2   1   1  0.24  0.65     100\n",
       "6950   6951   4  10   8   0   4   1   2  0.52  0.83     779\n",
       "\n",
       "[6951 rows x 11 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PS1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cdc1337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PS1B = PS1B.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1c1be3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanje\\AppData\\Local\\Temp\\ipykernel_12336\\3189199829.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_hat = mlp(torch.tensor(batch_X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 344.197, Validation Loss: 1368.165\n",
      "Epoch [2/150], Train Loss: 129.490, Validation Loss: 625.765\n",
      "Epoch [3/150], Train Loss: 94.940, Validation Loss: 493.836\n",
      "Epoch [4/150], Train Loss: 72.280, Validation Loss: 421.576\n",
      "Epoch [5/150], Train Loss: 59.933, Validation Loss: 371.599\n",
      "Epoch [6/150], Train Loss: 49.426, Validation Loss: 345.667\n",
      "Epoch [7/150], Train Loss: 46.475, Validation Loss: 329.510\n",
      "Epoch [8/150], Train Loss: 43.391, Validation Loss: 267.382\n",
      "Epoch [9/150], Train Loss: 42.523, Validation Loss: 255.053\n",
      "Epoch [10/150], Train Loss: 40.062, Validation Loss: 234.290\n",
      "Epoch [11/150], Train Loss: 38.057, Validation Loss: 229.854\n",
      "Epoch [12/150], Train Loss: 36.983, Validation Loss: 208.381\n",
      "Epoch [13/150], Train Loss: 35.554, Validation Loss: 200.423\n",
      "Epoch [14/150], Train Loss: 33.959, Validation Loss: 186.536\n",
      "Epoch [15/150], Train Loss: 32.917, Validation Loss: 192.349\n",
      "Epoch [16/150], Train Loss: 32.874, Validation Loss: 190.770\n",
      "Epoch [17/150], Train Loss: 32.285, Validation Loss: 194.489\n",
      "Epoch [18/150], Train Loss: 32.018, Validation Loss: 189.458\n",
      "Epoch [19/150], Train Loss: 31.226, Validation Loss: 190.693\n",
      "Epoch [20/150], Train Loss: 31.251, Validation Loss: 189.489\n",
      "Epoch [21/150], Train Loss: 30.654, Validation Loss: 188.507\n",
      "Epoch [22/150], Train Loss: 30.588, Validation Loss: 186.767\n",
      "Epoch [23/150], Train Loss: 29.879, Validation Loss: 183.807\n",
      "Epoch [24/150], Train Loss: 29.610, Validation Loss: 183.720\n",
      "Epoch [25/150], Train Loss: 29.201, Validation Loss: 180.934\n",
      "Epoch [26/150], Train Loss: 29.021, Validation Loss: 177.596\n",
      "Epoch [27/150], Train Loss: 28.665, Validation Loss: 174.783\n",
      "Epoch [28/150], Train Loss: 28.154, Validation Loss: 176.461\n",
      "Epoch [29/150], Train Loss: 28.247, Validation Loss: 172.686\n",
      "Epoch [30/150], Train Loss: 27.703, Validation Loss: 171.025\n",
      "Epoch [31/150], Train Loss: 27.589, Validation Loss: 168.914\n",
      "Epoch [32/150], Train Loss: 27.124, Validation Loss: 166.828\n",
      "Epoch [33/150], Train Loss: 26.997, Validation Loss: 169.310\n",
      "Epoch [34/150], Train Loss: 26.679, Validation Loss: 165.711\n",
      "Epoch [35/150], Train Loss: 26.367, Validation Loss: 165.914\n",
      "Epoch [36/150], Train Loss: 25.958, Validation Loss: 167.628\n",
      "Epoch [37/150], Train Loss: 25.762, Validation Loss: 168.170\n",
      "Epoch [38/150], Train Loss: 25.693, Validation Loss: 170.813\n",
      "Epoch [39/150], Train Loss: 25.702, Validation Loss: 170.144\n",
      "Epoch [40/150], Train Loss: 25.393, Validation Loss: 172.860\n",
      "Epoch [41/150], Train Loss: 25.462, Validation Loss: 169.471\n",
      "Epoch [42/150], Train Loss: 25.304, Validation Loss: 177.775\n",
      "Epoch [43/150], Train Loss: 25.729, Validation Loss: 179.348\n",
      "Epoch [44/150], Train Loss: 25.456, Validation Loss: 179.300\n",
      "Epoch [45/150], Train Loss: 25.621, Validation Loss: 191.849\n",
      "Epoch [46/150], Train Loss: 26.827, Validation Loss: 182.101\n",
      "Epoch [47/150], Train Loss: 29.014, Validation Loss: 167.163\n",
      "Epoch [48/150], Train Loss: 29.892, Validation Loss: 173.163\n",
      "Epoch [49/150], Train Loss: 29.149, Validation Loss: 158.126\n",
      "Epoch [50/150], Train Loss: 24.964, Validation Loss: 158.103\n",
      "Epoch [51/150], Train Loss: 23.450, Validation Loss: 150.067\n",
      "Epoch [52/150], Train Loss: 22.354, Validation Loss: 150.142\n",
      "Epoch [53/150], Train Loss: 22.045, Validation Loss: 153.934\n",
      "Epoch [54/150], Train Loss: 21.875, Validation Loss: 151.786\n",
      "Epoch [55/150], Train Loss: 23.107, Validation Loss: 150.110\n",
      "Epoch [56/150], Train Loss: 23.650, Validation Loss: 154.983\n",
      "Epoch [57/150], Train Loss: 23.681, Validation Loss: 151.153\n",
      "Epoch [58/150], Train Loss: 22.830, Validation Loss: 154.879\n",
      "Epoch [59/150], Train Loss: 22.293, Validation Loss: 145.810\n",
      "Epoch [60/150], Train Loss: 20.689, Validation Loss: 140.448\n",
      "Epoch [61/150], Train Loss: 20.798, Validation Loss: 148.390\n",
      "Epoch [62/150], Train Loss: 21.634, Validation Loss: 158.167\n",
      "Epoch [63/150], Train Loss: 21.164, Validation Loss: 152.615\n",
      "Epoch [64/150], Train Loss: 20.614, Validation Loss: 156.402\n",
      "Epoch [65/150], Train Loss: 20.428, Validation Loss: 148.994\n",
      "Epoch [66/150], Train Loss: 20.609, Validation Loss: 148.718\n",
      "Epoch [67/150], Train Loss: 20.594, Validation Loss: 147.542\n",
      "Epoch [68/150], Train Loss: 20.391, Validation Loss: 143.874\n",
      "Epoch [69/150], Train Loss: 19.746, Validation Loss: 141.585\n",
      "Epoch [70/150], Train Loss: 19.420, Validation Loss: 142.817\n",
      "Epoch [71/150], Train Loss: 19.470, Validation Loss: 144.905\n",
      "Epoch [72/150], Train Loss: 19.177, Validation Loss: 145.629\n",
      "Epoch [73/150], Train Loss: 18.860, Validation Loss: 157.928\n",
      "Epoch [74/150], Train Loss: 19.660, Validation Loss: 159.870\n",
      "Epoch [75/150], Train Loss: 19.673, Validation Loss: 146.575\n",
      "Epoch [76/150], Train Loss: 20.139, Validation Loss: 148.013\n",
      "Epoch [77/150], Train Loss: 20.224, Validation Loss: 154.468\n",
      "Epoch [78/150], Train Loss: 19.686, Validation Loss: 143.712\n",
      "Epoch [79/150], Train Loss: 18.956, Validation Loss: 135.106\n",
      "Epoch [80/150], Train Loss: 18.548, Validation Loss: 143.340\n",
      "Epoch [81/150], Train Loss: 19.249, Validation Loss: 157.176\n",
      "Epoch [82/150], Train Loss: 19.562, Validation Loss: 142.681\n",
      "Epoch [83/150], Train Loss: 18.229, Validation Loss: 148.442\n",
      "Epoch [84/150], Train Loss: 19.013, Validation Loss: 140.796\n",
      "Epoch [85/150], Train Loss: 18.579, Validation Loss: 141.233\n",
      "Epoch [86/150], Train Loss: 18.466, Validation Loss: 151.173\n",
      "Epoch [87/150], Train Loss: 18.857, Validation Loss: 152.152\n",
      "Epoch [88/150], Train Loss: 21.325, Validation Loss: 176.639\n",
      "Epoch [89/150], Train Loss: 23.111, Validation Loss: 171.774\n",
      "Epoch [90/150], Train Loss: 25.174, Validation Loss: 150.547\n",
      "Epoch [91/150], Train Loss: 24.545, Validation Loss: 142.985\n",
      "Epoch [92/150], Train Loss: 20.348, Validation Loss: 137.235\n",
      "Epoch [93/150], Train Loss: 17.713, Validation Loss: 139.426\n",
      "Epoch [94/150], Train Loss: 17.407, Validation Loss: 132.683\n",
      "Epoch [95/150], Train Loss: 17.183, Validation Loss: 134.260\n",
      "Epoch [96/150], Train Loss: 17.074, Validation Loss: 132.665\n",
      "Epoch [97/150], Train Loss: 16.946, Validation Loss: 135.731\n",
      "Epoch [98/150], Train Loss: 17.354, Validation Loss: 132.087\n",
      "Epoch [99/150], Train Loss: 17.435, Validation Loss: 129.350\n",
      "Epoch [100/150], Train Loss: 17.587, Validation Loss: 124.354\n",
      "Epoch [101/150], Train Loss: 17.516, Validation Loss: 122.822\n",
      "Epoch [102/150], Train Loss: 17.035, Validation Loss: 129.540\n",
      "Epoch [103/150], Train Loss: 17.260, Validation Loss: 129.270\n",
      "Epoch [104/150], Train Loss: 17.025, Validation Loss: 132.602\n",
      "Epoch [105/150], Train Loss: 16.461, Validation Loss: 131.573\n",
      "Epoch [106/150], Train Loss: 16.670, Validation Loss: 153.792\n",
      "Epoch [107/150], Train Loss: 17.592, Validation Loss: 148.109\n",
      "Epoch [108/150], Train Loss: 16.525, Validation Loss: 157.468\n",
      "Epoch [109/150], Train Loss: 18.160, Validation Loss: 156.279\n",
      "Epoch [110/150], Train Loss: 20.880, Validation Loss: 144.680\n",
      "Epoch [111/150], Train Loss: 20.524, Validation Loss: 143.098\n",
      "Epoch [112/150], Train Loss: 19.876, Validation Loss: 132.670\n",
      "Epoch [113/150], Train Loss: 18.001, Validation Loss: 139.663\n",
      "Epoch [114/150], Train Loss: 17.139, Validation Loss: 145.101\n",
      "Epoch [115/150], Train Loss: 16.291, Validation Loss: 151.663\n",
      "Epoch [116/150], Train Loss: 17.335, Validation Loss: 130.152\n",
      "Epoch [117/150], Train Loss: 17.869, Validation Loss: 127.582\n",
      "Epoch [118/150], Train Loss: 16.239, Validation Loss: 124.384\n",
      "Epoch [119/150], Train Loss: 16.581, Validation Loss: 149.223\n",
      "Epoch [120/150], Train Loss: 15.361, Validation Loss: 142.338\n",
      "Epoch [121/150], Train Loss: 14.810, Validation Loss: 121.435\n",
      "Epoch [122/150], Train Loss: 14.628, Validation Loss: 120.347\n",
      "Epoch [123/150], Train Loss: 13.720, Validation Loss: 116.949\n",
      "Epoch [124/150], Train Loss: 13.743, Validation Loss: 127.419\n",
      "Epoch [125/150], Train Loss: 13.626, Validation Loss: 132.577\n",
      "Epoch [126/150], Train Loss: 13.488, Validation Loss: 135.536\n",
      "Epoch [127/150], Train Loss: 14.556, Validation Loss: 126.453\n",
      "Epoch [128/150], Train Loss: 14.530, Validation Loss: 136.601\n",
      "Epoch [129/150], Train Loss: 15.696, Validation Loss: 132.384\n",
      "Epoch [130/150], Train Loss: 15.640, Validation Loss: 127.090\n",
      "Epoch [131/150], Train Loss: 15.516, Validation Loss: 125.405\n",
      "Epoch [132/150], Train Loss: 14.272, Validation Loss: 117.170\n",
      "Epoch [133/150], Train Loss: 14.235, Validation Loss: 128.734\n",
      "Epoch [134/150], Train Loss: 14.911, Validation Loss: 152.631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [135/150], Train Loss: 15.758, Validation Loss: 150.533\n",
      "Epoch [136/150], Train Loss: 14.627, Validation Loss: 166.422\n",
      "Epoch [137/150], Train Loss: 15.338, Validation Loss: 144.540\n",
      "Epoch [138/150], Train Loss: 16.541, Validation Loss: 130.380\n",
      "Epoch [139/150], Train Loss: 15.615, Validation Loss: 125.369\n",
      "Epoch [140/150], Train Loss: 13.806, Validation Loss: 117.781\n",
      "Epoch [141/150], Train Loss: 12.916, Validation Loss: 146.741\n",
      "Epoch [142/150], Train Loss: 13.091, Validation Loss: 139.991\n",
      "Epoch [143/150], Train Loss: 13.751, Validation Loss: 154.659\n",
      "Epoch [144/150], Train Loss: 13.954, Validation Loss: 122.262\n",
      "Epoch [145/150], Train Loss: 13.903, Validation Loss: 129.365\n",
      "Epoch [146/150], Train Loss: 13.176, Validation Loss: 133.594\n",
      "Epoch [147/150], Train Loss: 13.134, Validation Loss: 143.746\n",
      "Epoch [148/150], Train Loss: 12.453, Validation Loss: 128.899\n",
      "Epoch [149/150], Train Loss: 12.417, Validation Loss: 126.538\n",
      "Epoch [150/150], Train Loss: 13.134, Validation Loss: 123.303\n",
      "X1,  X2,  X3,  X4,  X5,  y\n",
      "0   -2   0   0   -1   0   0   0   0\n",
      "1   0   0   -1   -1   0   0   0   6\n",
      "0   0   -2   0   0   1   0   0   3\n",
      "0   0   0   1   0   0   0   0   -3\n",
      "0   0   1   -1   0   -1   0   0   2\n",
      "0   1   0   0   -1   0   0   0   0\n",
      "0   0   0   -2   0   1   0   0   6\n",
      "0   -1   1   -2   -1   0   0   -1   5\n",
      "2   1   0   -1   0   0   0   1   3\n",
      "1   -1   0   0   -1   0   0   0   0\n",
      "0   0   0   0   -1   2   0   0   7\n",
      "0   0   0   0   0   0   0   0   4\n",
      "0   0   0   0   0   0   0   0   3\n",
      "0   0   -1   0   0   0   1   0   1\n",
      "0   0   0   0   -1   0   0   0   3\n",
      "-1   0   0   0   0   0   0   -1   1\n",
      "1   -2   -2   0   0   0   1   1   4\n",
      "1   -1   0   1   1   -1   0   0   -1\n",
      "0   0   0   0   0   -1   0   0   1\n",
      "0   0   0   0   1   0   0   0   1\n",
      "0   0   0   -1   0   0   0   0   4\n",
      "0   0   0   0   0   0   0   0   4\n",
      "0   1   0   0   1   0   0   1   4\n",
      "0   0   1   0   0   1   0   0   3\n",
      "0   2   0   0   -2   0   0   -1   1\n",
      "0   2   0   0   0   1   1   0   2\n",
      "0   -2   0   2   1   0   0   0   0\n",
      "0   0   0   0   1   0   0   0   2\n",
      "0   0   0   0   1   1   2   1   3\n",
      "0   0   1   0   1   1   0   0   5\n",
      "0   0   -1   -1   0   0   0   1   3\n",
      "-2   0   0   0   0   0   -1   0   5\n",
      "1   -2   0   0   1   0   -1   0   0\n",
      "0   0   0   2   0   1   0   0   0\n",
      "0   0   0   0   0   0   0   0   3\n",
      "0   -1   1   -1   0   -1   0   1   2\n",
      "0   0   1   0   -1   0   0   0   2\n",
      "0   1   0   -1   0   0   -1   0   1\n",
      "-1   0   0   1   -1   -1   -1   0   -2\n",
      "0   0   0   0   0   -1   0   -1   0\n",
      "0   1   0   0   0   0   0   0   4\n",
      "1   0   0   -1   0   0   -1   0   3\n",
      "-1   0   0   0   0   0   0   -1   0\n",
      "1   0   0   0   0   0   0   -1   2\n",
      "0   -1   -2   -1   0   0   0   0   6\n",
      "0   1   0   0   0   1   1   0   3\n",
      "0   -1   0   1   0   2   1   0   3\n",
      "0   0   0   1   0   -1   0   0   -3\n",
      "-2   0   -2   0   0   0   0   0   3\n",
      "1   0   0   2   -1   0   0   1   0\n",
      "0   -1   -1   2   0   0   0   0   -3\n",
      "0   0   0   0   -1   0   0   0   2\n",
      "0   0   0   0   0   0   -1   0   0\n",
      "1   0   0   -2   0   0   1   0   5\n",
      "1   -1   -2   1   0   0   -1   0   1\n",
      "1   0   0   1   0   1   0   0   2\n",
      "-1   0   1   0   0   0   -1   -1   3\n",
      "0   -1   0   0   0   -1   0   0   -2\n",
      "0   0   0   2   0   0   -1   0   -2\n",
      "-1   -1   0   0   -2   0   -2   0   5\n",
      "0   0   -2   1   -1   0   0   0   -2\n",
      "-1   0   -1   1   0   -1   0   0   -3\n",
      "0   1   0   0   0   0   -1   0   4\n",
      "0   1   1   -1   0   0   0   0   3\n",
      "1   0   0   0   0   -1   0   0   1\n",
      "2   1   -1   0   1   0   1   0   3\n",
      "0   0   -1   0   -2   0   1   -1   4\n",
      "0   -1   0   0   0   0   -1   0   4\n",
      "0   0   2   1   -1   0   0   0   2\n",
      "0   0   0   1   0   0   0   -2   0\n",
      "0   1   0   0   0   1   1   0   4\n",
      "1   0   -2   0   0   0   2   0   2\n",
      "0   0   -1   0   0   1   0   1   4\n",
      "1   1   0   0   1   0   0   0   3\n",
      "0   0   0   -2   -1   0   0   0   5\n",
      "0   0   0   -2   0   0   -1   -1   6\n",
      "0   2   -1   -1   0   0   0   0   4\n",
      "0   0   0   0   0   1   0   0   3\n",
      "1   -3   0   0   0   0   0   0   1\n",
      "-2   -1   0   0   -1   0   0   1   2\n",
      "-1   1   0   0   0   1   1   1   4\n",
      "0   2   0   0   0   -1   0   0   0\n",
      "0   0   0   0   -1   0   0   0   1\n",
      "0   0   -1   0   0   0   0   -1   0\n",
      "-1   0   0   0   0   0   0   0   2\n",
      "-2   1   0   1   0   0   -1   -1   0\n",
      "0   0   0   0   0   0   0   0   2\n",
      "-1   0   3   0   0   0   0   1   3\n",
      "-1   1   0   0   0   0   1   1   0\n",
      "-1   0   2   0   0   0   -1   0   2\n",
      "0   0   0   0   1   0   0   1   2\n",
      "0   2   0   0   0   0   0   0   1\n",
      "0   -1   1   1   1   1   0   0   3\n",
      "0   0   0   0   0   0   0   0   2\n",
      "0   0   -1   0   0   0   0   -1   4\n",
      "0   0   0   0   1   0   0   0   3\n",
      "0   0   0   0   0   0   1   0   1\n",
      "0   0   0   -1   0   0   -1   0   5\n",
      "0   0   0   1   0   -2   -1   0   -2\n",
      "0   0   -2   1   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the hyperparameters\n",
    "n_samples = 1000\n",
    "n_features = 5\n",
    "hidden_size_1 = 256\n",
    "learning_rate = 0.01\n",
    "epochs = 150\n",
    "batch_size = 32\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, hidden_size_1)\n",
    "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_1)\n",
    "        self.fc3 = nn.Linear(hidden_size_1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = (self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(y_hat, y):\n",
    "    MSE = F.mse_loss(y_hat, y, reduction='sum')\n",
    "    return MSE\n",
    "\n",
    "# Generate synthetic data\n",
    "X = torch.tensor(np.array(PS1B.iloc[:, :9]))\n",
    "X = X.to(torch.float32)\n",
    "w_true = torch.randn(9, 1, dtype= torch.float32)\n",
    "b_true = torch.randn(1, 1, dtype=torch.float32)\n",
    "y = torch.mm(X, w_true) + b_true + torch.randn(len(X), 1, dtype=torch.float32)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_size = int(0.8 * n_samples)\n",
    "val_size = n_samples - train_size\n",
    "train_indices = torch.randperm(n_samples)[:train_size]\n",
    "val_indices = torch.randperm(n_samples)[train_size:]\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_val = X[val_indices]\n",
    "y_val = y[val_indices]\n",
    "\n",
    "# Initialize the MLP and optimizer\n",
    "mlp = MLP()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the MLP\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for batch_start in range(0, train_size, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, train_size)\n",
    "        batch_X = X_train[batch_start:batch_end].to(torch.float32)\n",
    "        batch_y = y_train[batch_start:batch_end]\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = mlp(torch.tensor(batch_X))\n",
    "        loss = loss_function(y_hat, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * (batch_end - batch_start)\n",
    "    train_loss /= train_size\n",
    "    train_losses.append(train_loss)\n",
    "    with torch.no_grad():\n",
    "        y_val_hat = mlp(X_val)\n",
    "        val_loss = loss_function(y_val_hat, y_val)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    print('Epoch [{}/{}], Train Loss: {:.3f}, Validation Loss: {:.3f}'.format(epoch+1, epochs, train_loss, val_loss.item()))\n",
    "\n",
    "# Generate synthetic data using the trained model\n",
    "X_test = torch.randn(len(X), 9, dtype=torch.float32)\n",
    "y_test = mlp(X_test).detach()\n",
    "\n",
    "# Print the first few rows of the synthetic data\n",
    "print('X1,  X2,  X3,  X4,  X5,  y')\n",
    "for i in range(100):\n",
    "    print(int(X_test[i, 1]), ' ',int(X_test[i, 2]),' ', int(X_test[i, 3]),' ', int(X_test[i, 4]),' ', int(X_test[i, 5]),' ', int(X_test[i, 6]), ' ',int(X_test[i, 7]),' ', int(X_test[i, 8]),' ',int(y_test[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0ed58015",
   "metadata": {},
   "outputs": [],
   "source": [
    "PS1C = pd.read_csv(\"PS1C_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac4671c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>...</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "      <th>C27</th>\n",
       "      <th>C28</th>\n",
       "      <th>C29</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.287744</td>\n",
       "      <td>0.222853</td>\n",
       "      <td>-2.998039</td>\n",
       "      <td>-0.690275</td>\n",
       "      <td>-0.674866</td>\n",
       "      <td>-0.404163</td>\n",
       "      <td>0.405966</td>\n",
       "      <td>-2.319937</td>\n",
       "      <td>0.775369</td>\n",
       "      <td>-0.057652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031281</td>\n",
       "      <td>0.395767</td>\n",
       "      <td>-0.097444</td>\n",
       "      <td>-0.357012</td>\n",
       "      <td>-0.230728</td>\n",
       "      <td>-0.239019</td>\n",
       "      <td>0.272303</td>\n",
       "      <td>0.128075</td>\n",
       "      <td>29.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.290936</td>\n",
       "      <td>1.081047</td>\n",
       "      <td>1.689179</td>\n",
       "      <td>1.857100</td>\n",
       "      <td>-0.633250</td>\n",
       "      <td>-0.808269</td>\n",
       "      <td>0.466850</td>\n",
       "      <td>-0.099381</td>\n",
       "      <td>0.161682</td>\n",
       "      <td>0.374637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233531</td>\n",
       "      <td>0.229839</td>\n",
       "      <td>0.367753</td>\n",
       "      <td>-0.345799</td>\n",
       "      <td>-0.614424</td>\n",
       "      <td>-0.322642</td>\n",
       "      <td>0.159902</td>\n",
       "      <td>0.221186</td>\n",
       "      <td>38.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.939394</td>\n",
       "      <td>1.391180</td>\n",
       "      <td>-1.432628</td>\n",
       "      <td>-1.598677</td>\n",
       "      <td>0.829657</td>\n",
       "      <td>1.103927</td>\n",
       "      <td>-0.276995</td>\n",
       "      <td>0.600442</td>\n",
       "      <td>-0.757897</td>\n",
       "      <td>-0.567842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161042</td>\n",
       "      <td>0.918642</td>\n",
       "      <td>0.382627</td>\n",
       "      <td>-0.575711</td>\n",
       "      <td>0.240417</td>\n",
       "      <td>-0.217267</td>\n",
       "      <td>0.191545</td>\n",
       "      <td>-0.014056</td>\n",
       "      <td>399.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.170442</td>\n",
       "      <td>-1.945039</td>\n",
       "      <td>0.273686</td>\n",
       "      <td>0.740743</td>\n",
       "      <td>-0.530378</td>\n",
       "      <td>0.199080</td>\n",
       "      <td>-0.220303</td>\n",
       "      <td>0.476406</td>\n",
       "      <td>-0.388658</td>\n",
       "      <td>-1.035835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.399412</td>\n",
       "      <td>-1.022962</td>\n",
       "      <td>0.261353</td>\n",
       "      <td>-0.051819</td>\n",
       "      <td>-0.165381</td>\n",
       "      <td>0.207375</td>\n",
       "      <td>-0.060959</td>\n",
       "      <td>-0.035671</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.362427</td>\n",
       "      <td>-0.280416</td>\n",
       "      <td>-1.437635</td>\n",
       "      <td>-1.381315</td>\n",
       "      <td>-0.249888</td>\n",
       "      <td>-1.454115</td>\n",
       "      <td>-0.136940</td>\n",
       "      <td>-0.081281</td>\n",
       "      <td>1.275863</td>\n",
       "      <td>1.595319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.496381</td>\n",
       "      <td>-0.743399</td>\n",
       "      <td>0.391387</td>\n",
       "      <td>-0.437921</td>\n",
       "      <td>-0.595208</td>\n",
       "      <td>-0.528237</td>\n",
       "      <td>0.009113</td>\n",
       "      <td>-0.047769</td>\n",
       "      <td>39.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.675362</td>\n",
       "      <td>-1.441520</td>\n",
       "      <td>-0.488052</td>\n",
       "      <td>-0.143098</td>\n",
       "      <td>-0.451366</td>\n",
       "      <td>-0.183787</td>\n",
       "      <td>-0.273587</td>\n",
       "      <td>-0.565989</td>\n",
       "      <td>0.796306</td>\n",
       "      <td>-1.167151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.605608</td>\n",
       "      <td>-1.179861</td>\n",
       "      <td>0.332523</td>\n",
       "      <td>0.413023</td>\n",
       "      <td>-0.270728</td>\n",
       "      <td>0.374114</td>\n",
       "      <td>-0.060189</td>\n",
       "      <td>-0.048870</td>\n",
       "      <td>40.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.653237</td>\n",
       "      <td>0.422112</td>\n",
       "      <td>2.385177</td>\n",
       "      <td>0.102150</td>\n",
       "      <td>-0.474317</td>\n",
       "      <td>0.335611</td>\n",
       "      <td>-0.123559</td>\n",
       "      <td>-0.851044</td>\n",
       "      <td>0.672238</td>\n",
       "      <td>-0.776345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.508938</td>\n",
       "      <td>-1.628332</td>\n",
       "      <td>0.217964</td>\n",
       "      <td>-0.063715</td>\n",
       "      <td>0.168564</td>\n",
       "      <td>-0.468109</td>\n",
       "      <td>-0.023746</td>\n",
       "      <td>0.026011</td>\n",
       "      <td>14.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.322419</td>\n",
       "      <td>0.501951</td>\n",
       "      <td>1.033746</td>\n",
       "      <td>-0.317320</td>\n",
       "      <td>0.431012</td>\n",
       "      <td>-0.323396</td>\n",
       "      <td>0.054104</td>\n",
       "      <td>0.461689</td>\n",
       "      <td>-0.075649</td>\n",
       "      <td>-1.596593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101561</td>\n",
       "      <td>0.054401</td>\n",
       "      <td>-0.307052</td>\n",
       "      <td>-0.993344</td>\n",
       "      <td>0.495802</td>\n",
       "      <td>-0.291367</td>\n",
       "      <td>0.038254</td>\n",
       "      <td>0.051725</td>\n",
       "      <td>144.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.330676</td>\n",
       "      <td>-0.372405</td>\n",
       "      <td>1.134086</td>\n",
       "      <td>-0.371052</td>\n",
       "      <td>0.236852</td>\n",
       "      <td>-0.676891</td>\n",
       "      <td>0.246508</td>\n",
       "      <td>0.766768</td>\n",
       "      <td>0.394456</td>\n",
       "      <td>0.669769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281381</td>\n",
       "      <td>0.803754</td>\n",
       "      <td>0.162364</td>\n",
       "      <td>0.663850</td>\n",
       "      <td>-0.158224</td>\n",
       "      <td>-0.611218</td>\n",
       "      <td>0.041129</td>\n",
       "      <td>-0.031165</td>\n",
       "      <td>29.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.182008</td>\n",
       "      <td>0.231718</td>\n",
       "      <td>1.230734</td>\n",
       "      <td>-0.100173</td>\n",
       "      <td>-0.441451</td>\n",
       "      <td>0.233417</td>\n",
       "      <td>-0.117236</td>\n",
       "      <td>-0.129034</td>\n",
       "      <td>-0.006021</td>\n",
       "      <td>-0.142516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090599</td>\n",
       "      <td>0.224125</td>\n",
       "      <td>-0.118463</td>\n",
       "      <td>0.094806</td>\n",
       "      <td>0.596188</td>\n",
       "      <td>-0.289625</td>\n",
       "      <td>0.019921</td>\n",
       "      <td>0.027253</td>\n",
       "      <td>59.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           C2        C3        C4        C5        C6        C7        C8  \\\n",
       "0   -0.287744  0.222853 -2.998039 -0.690275 -0.674866 -0.404163  0.405966   \n",
       "1   -1.290936  1.081047  1.689179  1.857100 -0.633250 -0.808269  0.466850   \n",
       "2   -0.939394  1.391180 -1.432628 -1.598677  0.829657  1.103927 -0.276995   \n",
       "3    0.170442 -1.945039  0.273686  0.740743 -0.530378  0.199080 -0.220303   \n",
       "4   -1.362427 -0.280416 -1.437635 -1.381315 -0.249888 -1.454115 -0.136940   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995 -0.675362 -1.441520 -0.488052 -0.143098 -0.451366 -0.183787 -0.273587   \n",
       "996  0.653237  0.422112  2.385177  0.102150 -0.474317  0.335611 -0.123559   \n",
       "997 -0.322419  0.501951  1.033746 -0.317320  0.431012 -0.323396  0.054104   \n",
       "998 -0.330676 -0.372405  1.134086 -0.371052  0.236852 -0.676891  0.246508   \n",
       "999  0.182008  0.231718  1.230734 -0.100173 -0.441451  0.233417 -0.117236   \n",
       "\n",
       "           C9       C10       C11  ...       C21       C22       C23  \\\n",
       "0   -2.319937  0.775369 -0.057652  ...  0.031281  0.395767 -0.097444   \n",
       "1   -0.099381  0.161682  0.374637  ...  0.233531  0.229839  0.367753   \n",
       "2    0.600442 -0.757897 -0.567842  ...  0.161042  0.918642  0.382627   \n",
       "3    0.476406 -0.388658 -1.035835  ... -0.399412 -1.022962  0.261353   \n",
       "4   -0.081281  1.275863  1.595319  ... -0.496381 -0.743399  0.391387   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995 -0.565989  0.796306 -1.167151  ... -0.605608 -1.179861  0.332523   \n",
       "996 -0.851044  0.672238 -0.776345  ... -0.508938 -1.628332  0.217964   \n",
       "997  0.461689 -0.075649 -1.596593  ...  0.101561  0.054401 -0.307052   \n",
       "998  0.766768  0.394456  0.669769  ...  0.281381  0.803754  0.162364   \n",
       "999 -0.129034 -0.006021 -0.142516  ...  0.090599  0.224125 -0.118463   \n",
       "\n",
       "          C24       C25       C26       C27       C28     C29  Target  \n",
       "0   -0.357012 -0.230728 -0.239019  0.272303  0.128075   29.89       0  \n",
       "1   -0.345799 -0.614424 -0.322642  0.159902  0.221186   38.94       0  \n",
       "2   -0.575711  0.240417 -0.217267  0.191545 -0.014056  399.40       0  \n",
       "3   -0.051819 -0.165381  0.207375 -0.060959 -0.035671    1.79       0  \n",
       "4   -0.437921 -0.595208 -0.528237  0.009113 -0.047769   39.95       0  \n",
       "..        ...       ...       ...       ...       ...     ...     ...  \n",
       "995  0.413023 -0.270728  0.374114 -0.060189 -0.048870   40.47       0  \n",
       "996 -0.063715  0.168564 -0.468109 -0.023746  0.026011   14.95       0  \n",
       "997 -0.993344  0.495802 -0.291367  0.038254  0.051725  144.00       0  \n",
       "998  0.663850 -0.158224 -0.611218  0.041129 -0.031165   29.00       0  \n",
       "999  0.094806  0.596188 -0.289625  0.019921  0.027253   59.70       0  \n",
       "\n",
       "[1000 rows x 29 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PS1C = PS1C.iloc[:, 1:]\n",
    "PS1C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a2df9054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanje\\AppData\\Local\\Temp\\ipykernel_12336\\3738104898.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_hat = mlp(torch.tensor(batch_X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 39521079.280, Validation Loss: 251578224.000\n",
      "Epoch [2/5], Train Loss: 39521007.920, Validation Loss: 251578224.000\n",
      "Epoch [3/5], Train Loss: 39521008.000, Validation Loss: 251578224.000\n",
      "Epoch [4/5], Train Loss: 39521007.600, Validation Loss: 251578224.000\n",
      "Epoch [5/5], Train Loss: 39521007.440, Validation Loss: 251578224.000\n",
      "X1,  X2,  X3,  X4,  X5,  y\n",
      "0  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "round(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mint\u001b[39m(X_test[i, j]), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,  )\n\u001b[1;32m---> 92\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: round(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the hyperparameters\n",
    "n_samples = 1000\n",
    "n_features = 5\n",
    "hidden_size_1 = 256\n",
    "hidden_size_2 = 256\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(29, hidden_size_1)\n",
    "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.fc3 = nn.Linear(hidden_size_2, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(y_hat, y):\n",
    "    MSE = F.mse_loss(y_hat, y, reduction='sum')\n",
    "    return MSE\n",
    "\n",
    "# Generate synthetic data\n",
    "X = torch.tensor(np.array(PS1C.iloc[:, :29]))\n",
    "X = X.to(torch.float32)\n",
    "w_true = torch.randn(29, 1, dtype= torch.float32)\n",
    "b_true = torch.randn(1, 1, dtype=torch.float32)\n",
    "y = torch.mm(X, w_true) + b_true + torch.randn(len(X), 1, dtype=torch.float32)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_size = int(0.8 * n_samples)\n",
    "val_size = n_samples - train_size\n",
    "train_indices = torch.randperm(n_samples)[:train_size]\n",
    "val_indices = torch.randperm(n_samples)[train_size:]\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_val = X[val_indices]\n",
    "y_val = y[val_indices]\n",
    "\n",
    "# Initialize the MLP and optimizer\n",
    "mlp = MLP()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the MLP\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for batch_start in range(0, train_size, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, train_size)\n",
    "        batch_X = X_train[batch_start:batch_end].to(torch.float32)\n",
    "        batch_y = y_train[batch_start:batch_end]\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = mlp(torch.tensor(batch_X))\n",
    "        loss = loss_function(y_hat, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * (batch_end - batch_start)\n",
    "    train_loss /= train_size\n",
    "    train_losses.append(train_loss)\n",
    "    with torch.no_grad():\n",
    "        y_val_hat = mlp(X_val)\n",
    "        val_loss = loss_function(y_val_hat, y_val)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    print('Epoch [{}/{}], Train Loss: {:.3f}, Validation Loss: {:.3f}'.format(epoch+1, epochs, train_loss, val_loss.item()))\n",
    "\n",
    "# Generate synthetic data using the trained model\n",
    "X_test = torch.randn(len(X), 29, dtype=torch.float32)\n",
    "y_test = mlp(X_test).detach()\n",
    "\n",
    "# Print the first few rows of the synthetic data\n",
    "print('X1,  X2,  X3,  X4,  X5,  y')\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        print(int(X_test[i, j]), ' ',  )\n",
    "        torch.round([y_test[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "26eaf2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PS1D = pd.read_csv(\"PS1D_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c43511f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PS1D = PS1D.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1323988f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1.2975</td>\n",
       "      <td>0.6075</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.3150</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.7875</td>\n",
       "      <td>0.3395</td>\n",
       "      <td>0.2005</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.155</td>\n",
       "      <td>1.0485</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.110</td>\n",
       "      <td>1.1360</td>\n",
       "      <td>0.5265</td>\n",
       "      <td>0.1915</td>\n",
       "      <td>0.2925</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0575</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>3</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.1075</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>2</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.8320</td>\n",
       "      <td>0.4355</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.2010</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>2</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.7170</td>\n",
       "      <td>0.2890</td>\n",
       "      <td>0.1745</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>3</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.7020</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>0.1670</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>2</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.3675</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>835 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     C1     C2     C3     C4      C5      C6      C7      C8  Target\n",
       "0     1  0.665  0.500  0.175  1.2975  0.6075  0.3140  0.3150       9\n",
       "1     2  0.535  0.460  0.145  0.7875  0.3395  0.2005  0.2000       8\n",
       "2     1  0.625  0.495  0.155  1.0485  0.4870  0.2120  0.3215      11\n",
       "3     2  0.625  0.490  0.110  1.1360  0.5265  0.1915  0.2925       9\n",
       "4     3  0.410  0.300  0.090  0.2800  0.1410  0.0575  0.0750       8\n",
       "..   ..    ...    ...    ...     ...     ...     ...     ...     ...\n",
       "830   3  0.365  0.295  0.095  0.2500  0.1075  0.0545  0.0800       9\n",
       "831   2  0.545  0.430  0.140  0.8320  0.4355  0.1700  0.2010       9\n",
       "832   2  0.525  0.430  0.165  0.7170  0.2890  0.1745  0.1950      10\n",
       "833   3  0.540  0.415  0.155  0.7020  0.3220  0.1670  0.1900      10\n",
       "834   2  0.415  0.340  0.130  0.3675  0.1460  0.0885  0.1200      10\n",
       "\n",
       "[835 rows x 9 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PS1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "49e8f7bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 897 is out of bounds for dimension 0 with size 835",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m train_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(n_samples)[:train_size]\n\u001b[0;32m     49\u001b[0m val_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(n_samples)[train_size:]\n\u001b[1;32m---> 50\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     51\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y[train_indices]\n\u001b[0;32m     52\u001b[0m X_val \u001b[38;5;241m=\u001b[39m X[val_indices]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 897 is out of bounds for dimension 0 with size 835"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the hyperparameters\n",
    "n_samples = 1000\n",
    "n_features = 5\n",
    "hidden_size_1 = 256\n",
    "hidden_size_2 = 256\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, hidden_size_1)\n",
    "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.fc3 = nn.Linear(hidden_size_2, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(y_hat, y):\n",
    "    MSE = F.mse_loss(y_hat, y, reduction='sum')\n",
    "    return MSE\n",
    "\n",
    "# Generate synthetic data\n",
    "X = torch.tensor(np.array(PS1D.iloc[:, :8]))\n",
    "X = X.to(torch.float32)\n",
    "w_true = torch.randn(8, 1, dtype= torch.float32)\n",
    "b_true = torch.randn(1, 1, dtype=torch.float32)\n",
    "y = torch.mm(X, w_true) + b_true + torch.randn(len(X), 1, dtype=torch.float32)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_size = int(0.8 * len(X))\n",
    "val_size = n_samples - train_size\n",
    "train_indices = torch.randperm(n_samples)[:train_size]\n",
    "val_indices = torch.randperm(n_samples)[train_size:]\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_val = X[val_indices]\n",
    "y_val = y[val_indices]\n",
    "\n",
    "# Initialize the MLP and optimizer\n",
    "mlp = MLP()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the MLP\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for batch_start in range(0, train_size, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, train_size)\n",
    "        batch_X = X_train[batch_start:batch_end].to(torch.float32)\n",
    "        batch_y = y_train[batch_start:batch_end]\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = mlp(torch.tensor(batch_X))\n",
    "        loss = loss_function(y_hat, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * (batch_end - batch_start)\n",
    "    train_loss /= train_size\n",
    "    train_losses.append(train_loss)\n",
    "    with torch.no_grad():\n",
    "        y_val_hat = mlp(X_val)\n",
    "        val_loss = loss_function(y_val_hat, y_val)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    print('Epoch [{}/{}], Train Loss: {:.3f}, Validation Loss: {:.3f}'.format(epoch+1, epochs, train_loss, val_loss.item()))\n",
    "\n",
    "# Generate synthetic data using the trained model\n",
    "X_test = torch.randn(len(X), 8, dtype=torch.float32)\n",
    "y_test = mlp(X_test).detach()\n",
    "\n",
    "# Print the first few rows of the synthetic data\n",
    "print('X1,  X2,  X3,  X4,  X5,  y')\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        print(int(X_test[i, j]), ' ',  )\n",
    "        torch.round([y_test[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710bd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
